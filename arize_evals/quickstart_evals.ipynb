{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8432e8a-a062-46f5-8dba-21d30a6341e2",
   "metadata": {},
   "source": [
    "# https://docs.arize.com/phoenix/evaluation/evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a576937e-fdaa-462e-9745-6ad85631e7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Eiffel Tower is located in Paris, France. ...</td>\n",
       "      <td>Where is the Eiffel Tower located?</td>\n",
       "      <td>The Eiffel Tower is located in Paris, France.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Great Wall of China is over 13,000 miles l...</td>\n",
       "      <td>How long is the Great Wall of China?</td>\n",
       "      <td>The Great Wall of China is approximately 13,17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Amazon rainforest is the largest tropical ...</td>\n",
       "      <td>What is the largest tropical rainforest?</td>\n",
       "      <td>The Amazon rainforest is the largest tropical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mount Everest is the highest mountain on Earth...</td>\n",
       "      <td>Which is the highest mountain on Earth?</td>\n",
       "      <td>Mount Everest, standing at 29,029 feet (8,848 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Nile is the longest river in the world. It...</td>\n",
       "      <td>What is the longest river in the world?</td>\n",
       "      <td>The Nile River, at 6,650 kilometers (4,132 mil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reference  \\\n",
       "0  The Eiffel Tower is located in Paris, France. ...   \n",
       "1  The Great Wall of China is over 13,000 miles l...   \n",
       "2  The Amazon rainforest is the largest tropical ...   \n",
       "3  Mount Everest is the highest mountain on Earth...   \n",
       "4  The Nile is the longest river in the world. It...   \n",
       "\n",
       "                                      query  \\\n",
       "0        Where is the Eiffel Tower located?   \n",
       "1      How long is the Great Wall of China?   \n",
       "2  What is the largest tropical rainforest?   \n",
       "3   Which is the highest mountain on Earth?   \n",
       "4   What is the longest river in the world?   \n",
       "\n",
       "                                            response  \n",
       "0      The Eiffel Tower is located in Paris, France.  \n",
       "1  The Great Wall of China is approximately 13,17...  \n",
       "2  The Amazon rainforest is the largest tropical ...  \n",
       "3  Mount Everest, standing at 29,029 feet (8,848 ...  \n",
       "4  The Nile River, at 6,650 kilometers (4,132 mil...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"reference\": \"The Eiffel Tower is located in Paris, France. It was constructed in 1889 as the entrance arch to the 1889 World's Fair.\",\n",
    "            \"query\": \"Where is the Eiffel Tower located?\",\n",
    "            \"response\": \"The Eiffel Tower is located in Paris, France.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"The Great Wall of China is over 13,000 miles long. It was built over many centuries by various Chinese dynasties to protect against nomadic invasions.\",\n",
    "            \"query\": \"How long is the Great Wall of China?\",\n",
    "            \"response\": \"The Great Wall of China is approximately 13,171 miles (21,196 kilometers) long.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries.\",\n",
    "            \"query\": \"What is the largest tropical rainforest?\",\n",
    "            \"response\": \"The Amazon rainforest is the largest tropical rainforest in the world. It is home to the largest number of plant and animal species in the world.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"Mount Everest is the highest mountain on Earth. It is located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet.\",\n",
    "            \"query\": \"Which is the highest mountain on Earth?\",\n",
    "            \"response\": \"Mount Everest, standing at 29,029 feet (8,848 meters), is the highest mountain on Earth.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"The Nile is the longest river in the world. It flows northward through northeastern Africa for approximately 6,650 km (4,132 miles) from its most distant source in Burundi to the Mediterranean Sea.\",\n",
    "            \"query\": \"What is the longest river in the world?\",\n",
    "            \"response\": \"The Nile River, at 6,650 kilometers (4,132 miles), is the longest river in the world.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"The Mona Lisa was painted by Leonardo da Vinci. It is considered an archetypal masterpiece of the Italian Renaissance and has been described as 'the best known, the most visited, the most written about, the most sung about, the most parodied work of art in the world'.\",\n",
    "            \"query\": \"Who painted the Mona Lisa?\",\n",
    "            \"response\": \"The Mona Lisa was painted by the Italian Renaissance artist Leonardo da Vinci.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"The human body has 206 bones. These bones provide structure, protect organs, anchor muscles, and store calcium.\",\n",
    "            \"query\": \"How many bones are in the human body?\",\n",
    "            \"response\": \"The adult human body typically has 256 bones.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"Jupiter is the largest planet in our solar system. It is a gas giant with a mass more than two and a half times that of all the other planets in the solar system combined.\",\n",
    "            \"query\": \"Which planet is the largest in our solar system?\",\n",
    "            \"response\": \"Jupiter is the largest planet in our solar system.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"William Shakespeare wrote 'Romeo and Juliet'. It is a tragedy about two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\",\n",
    "            \"query\": \"Who wrote 'Romeo and Juliet'?\",\n",
    "            \"response\": \"The play 'Romeo and Juliet' was written by William Shakespeare.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"The first moon landing occurred in 1969. On July 20, 1969, American astronauts Neil Armstrong and Edwin 'Buzz' Aldrin became the first humans to land on the moon as part of the Apollo 11 mission.\",\n",
    "            \"query\": \"When did the first moon landing occur?\",\n",
    "            \"response\": \"The first moon landing took place on July 20, 1969.\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d12e8e41-f605-46d3-b9b0-430f10913bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1230ed8f41de4477af9fc04429f1f10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from phoenix.evals import HallucinationEvaluator, OpenAIModel, QAEvaluator, run_evals\n",
    "\n",
    "nest_asyncio.apply()  # This is needed for concurrency in notebook environments\n",
    "\n",
    "# Set your OpenAI API key\n",
    "eval_model = OpenAIModel(model=\"gpt-4o\")\n",
    "\n",
    "# Define your evaluators\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_evaluator = QAEvaluator(eval_model)\n",
    "\n",
    "# We have to make some minor changes to our dataframe to use the column names expected by our evaluators\n",
    "# for `hallucination_evaluator` the input df needs to have columns 'output', 'input', 'context'\n",
    "# for `qa_evaluator` the input df needs to have columns 'output', 'input', 'reference'\n",
    "df[\"context\"] = df[\"reference\"]\n",
    "df.rename(columns={\"query\": \"input\", \"response\": \"output\"}, inplace=True)\n",
    "assert all(column in df.columns for column in [\"output\", \"input\", \"context\", \"reference\"])\n",
    "\n",
    "# Run the evaluators, each evaluator will return a dataframe with evaluation results\n",
    "# We upload the evaluation results to Phoenix in the next step\n",
    "hallucination_eval_df, qa_eval_df = run_evals(\n",
    "    dataframe=df, evaluators=[hallucination_evaluator, qa_evaluator], provide_explanation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf99ad13-e001-4150-9f4f-a2babdc7cc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>context</th>\n",
       "      <th>hallucination_eval</th>\n",
       "      <th>hallucination_explanation</th>\n",
       "      <th>qa_eval</th>\n",
       "      <th>qa_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Eiffel Tower is located in Paris, France. It was constructed in 1889 as the entrance arch to the 1889 World's Fair.</td>\n",
       "      <td>Where is the Eiffel Tower located?</td>\n",
       "      <td>The Eiffel Tower is located in Paris, France.</td>\n",
       "      <td>The Eiffel Tower is located in Paris, France. It was constructed in 1889 as the entrance arch to the 1889 World's Fair.</td>\n",
       "      <td>factual</td>\n",
       "      <td>To determine if the answer is factual or hallucinated, we first look at the query, which asks about the location of the Eiffel Tower. The reference text provides the information that the Eiffel Tower is located in Paris, France. The answer given is \"The Eiffel Tower is located in Paris, France.\" This answer directly matches the information provided in the reference text. There is no additional information or deviation from the reference text in the answer. Therefore, the answer is consistent with the reference text and does not contain any hallucinated or made-up information.</td>\n",
       "      <td>correct</td>\n",
       "      <td>To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. \\n\\n1. The question asks for the location of the Eiffel Tower.\\n2. The reference text states that \"The Eiffel Tower is located in Paris, France.\"\\n3. The answer provided is \"The Eiffel Tower is located in Paris, France.\"\\n\\nThe answer matches exactly with the information given in the reference text regarding the location of the Eiffel Tower. Therefore, the answer correctly and fully answers the question based on the reference text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Great Wall of China is over 13,000 miles long. It was built over many centuries by various Chinese dynasties to protect against nomadic invasions.</td>\n",
       "      <td>How long is the Great Wall of China?</td>\n",
       "      <td>The Great Wall of China is approximately 13,171 miles (21,196 kilometers) long.</td>\n",
       "      <td>The Great Wall of China is over 13,000 miles long. It was built over many centuries by various Chinese dynasties to protect against nomadic invasions.</td>\n",
       "      <td>hallucinated</td>\n",
       "      <td>To determine if the answer is factual or hallucinated, we need to compare the information provided in the answer with the reference text. The reference text states that the Great Wall of China is over 13,000 miles long. The answer specifies that the Great Wall is approximately 13,171 miles (21,196 kilometers) long. The reference text does not provide an exact figure, only stating that it is over 13,000 miles. The answer provides a specific length that is not mentioned in the reference text. Since the reference text does not confirm the exact length given in the answer, the answer is considered to be a hallucination of facts, as it assumes information not available in the reference text.</td>\n",
       "      <td>correct</td>\n",
       "      <td>To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. \\n\\n1. The question asks about the length of the Great Wall of China.\\n2. The reference text states that the Great Wall of China is over 13,000 miles long.\\n3. The answer states that the Great Wall of China is approximately 13,171 miles (21,196 kilometers) long.\\n4. The answer provides a more precise figure (13,171 miles) which is consistent with the reference text's statement that the wall is over 13,000 miles long.\\n5. The reference text does not contradict the answer, and the answer provides additional detail that is plausible and aligns with the reference.\\n\\nTherefore, the answer correctly answers the question based on the reference text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries.</td>\n",
       "      <td>What is the largest tropical rainforest?</td>\n",
       "      <td>The Amazon rainforest is the largest tropical rainforest in the world. It is home to the largest number of plant and animal species in the world.</td>\n",
       "      <td>The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries.</td>\n",
       "      <td>hallucinated</td>\n",
       "      <td>To determine if the answer is factual or hallucinated, we need to compare the information in the answer with the reference text. The query asks about the largest tropical rainforest. The reference text states that the Amazon rainforest is the largest tropical rainforest in the world, which matches the first part of the answer. However, the answer also claims that the Amazon rainforest is home to the largest number of plant and animal species in the world. This information is not mentioned in the reference text. The reference text only provides information about the size and location of the Amazon rainforest, not about the biodiversity or the number of species it contains. Therefore, the claim about the largest number of plant and animal species is not supported by the reference text and is considered a hallucination.</td>\n",
       "      <td>correct</td>\n",
       "      <td>To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. The question asks for the largest tropical rainforest. The reference text states that the Amazon rainforest is the largest tropical rainforest in the world. The answer also states that the Amazon rainforest is the largest tropical rainforest in the world, which directly matches the information in the reference text. Additionally, the answer provides extra information about the Amazon rainforest being home to the largest number of plant and animal species, but this does not contradict the reference text or the question. Therefore, the answer correctly and fully answers the question based on the reference text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mount Everest is the highest mountain on Earth. It is located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet.</td>\n",
       "      <td>Which is the highest mountain on Earth?</td>\n",
       "      <td>Mount Everest, standing at 29,029 feet (8,848 meters), is the highest mountain on Earth.</td>\n",
       "      <td>Mount Everest is the highest mountain on Earth. It is located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet.</td>\n",
       "      <td>hallucinated</td>\n",
       "      <td>First, we identify the query, which asks for the highest mountain on Earth. The reference text clearly states that Mount Everest is the highest mountain on Earth, located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet. The answer provided states that Mount Everest is the highest mountain on Earth and adds specific details about its height, 29,029 feet (8,848 meters). The reference text does not provide the height of Mount Everest, so the specific height mentioned in the answer is not supported by the reference text. Therefore, the answer includes information that is not present in the reference text, making it a hallucination.</td>\n",
       "      <td>correct</td>\n",
       "      <td>To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. The question asks for the highest mountain on Earth. The reference text states that Mount Everest is the highest mountain on Earth. The answer also states that Mount Everest is the highest mountain on Earth and provides additional information about its height, which is consistent with known data. The additional detail about the height does not contradict the reference text and is accurate. Therefore, the answer correctly identifies Mount Everest as the highest mountain on Earth, which is consistent with the reference text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Nile is the longest river in the world. It flows northward through northeastern Africa for approximately 6,650 km (4,132 miles) from its most distant source in Burundi to the Mediterranean Sea.</td>\n",
       "      <td>What is the longest river in the world?</td>\n",
       "      <td>The Nile River, at 6,650 kilometers (4,132 miles), is the longest river in the world.</td>\n",
       "      <td>The Nile is the longest river in the world. It flows northward through northeastern Africa for approximately 6,650 km (4,132 miles) from its most distant source in Burundi to the Mediterranean Sea.</td>\n",
       "      <td>factual</td>\n",
       "      <td>To determine if the answer is factual or hallucinated, we need to compare the information in the answer with the reference text. The query asks for the longest river in the world. The reference text states that the Nile is the longest river in the world, flowing for approximately 6,650 km (4,132 miles). The answer also states that the Nile River is the longest river in the world and provides the same length of 6,650 kilometers (4,132 miles). Since the answer matches the information provided in the reference text, it does not contain any false information or assumptions not supported by the reference text. Therefore, the answer is factual.</td>\n",
       "      <td>correct</td>\n",
       "      <td>To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. \\n\\n1. The question asks for the longest river in the world.\\n2. The reference text states that the Nile is the longest river in the world, with a length of approximately 6,650 km (4,132 miles).\\n3. The answer states that the Nile River, at 6,650 kilometers (4,132 miles), is the longest river in the world.\\n\\nThe answer correctly identifies the Nile River as the longest river in the world and provides the same length as mentioned in the reference text. Therefore, the answer is consistent with the reference text and fully answers the question.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                               reference  \\\n",
       "0                                                                                The Eiffel Tower is located in Paris, France. It was constructed in 1889 as the entrance arch to the 1889 World's Fair.   \n",
       "1                                                 The Great Wall of China is over 13,000 miles long. It was built over many centuries by various Chinese dynasties to protect against nomadic invasions.   \n",
       "2                       The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries.   \n",
       "3                                       Mount Everest is the highest mountain on Earth. It is located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet.   \n",
       "4  The Nile is the longest river in the world. It flows northward through northeastern Africa for approximately 6,650 km (4,132 miles) from its most distant source in Burundi to the Mediterranean Sea.   \n",
       "\n",
       "                                      input  \\\n",
       "0        Where is the Eiffel Tower located?   \n",
       "1      How long is the Great Wall of China?   \n",
       "2  What is the largest tropical rainforest?   \n",
       "3   Which is the highest mountain on Earth?   \n",
       "4   What is the longest river in the world?   \n",
       "\n",
       "                                                                                                                                              output  \\\n",
       "0                                                                                                      The Eiffel Tower is located in Paris, France.   \n",
       "1                                                                    The Great Wall of China is approximately 13,171 miles (21,196 kilometers) long.   \n",
       "2  The Amazon rainforest is the largest tropical rainforest in the world. It is home to the largest number of plant and animal species in the world.   \n",
       "3                                                           Mount Everest, standing at 29,029 feet (8,848 meters), is the highest mountain on Earth.   \n",
       "4                                                              The Nile River, at 6,650 kilometers (4,132 miles), is the longest river in the world.   \n",
       "\n",
       "                                                                                                                                                                                                 context  \\\n",
       "0                                                                                The Eiffel Tower is located in Paris, France. It was constructed in 1889 as the entrance arch to the 1889 World's Fair.   \n",
       "1                                                 The Great Wall of China is over 13,000 miles long. It was built over many centuries by various Chinese dynasties to protect against nomadic invasions.   \n",
       "2                       The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries.   \n",
       "3                                       Mount Everest is the highest mountain on Earth. It is located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet.   \n",
       "4  The Nile is the longest river in the world. It flows northward through northeastern Africa for approximately 6,650 km (4,132 miles) from its most distant source in Burundi to the Mediterranean Sea.   \n",
       "\n",
       "  hallucination_eval  \\\n",
       "0            factual   \n",
       "1       hallucinated   \n",
       "2       hallucinated   \n",
       "3       hallucinated   \n",
       "4            factual   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      hallucination_explanation  \\\n",
       "0                                                                                                                                                                                                                                                        To determine if the answer is factual or hallucinated, we first look at the query, which asks about the location of the Eiffel Tower. The reference text provides the information that the Eiffel Tower is located in Paris, France. The answer given is \"The Eiffel Tower is located in Paris, France.\" This answer directly matches the information provided in the reference text. There is no additional information or deviation from the reference text in the answer. Therefore, the answer is consistent with the reference text and does not contain any hallucinated or made-up information.   \n",
       "1                                                                                                                                       To determine if the answer is factual or hallucinated, we need to compare the information provided in the answer with the reference text. The reference text states that the Great Wall of China is over 13,000 miles long. The answer specifies that the Great Wall is approximately 13,171 miles (21,196 kilometers) long. The reference text does not provide an exact figure, only stating that it is over 13,000 miles. The answer provides a specific length that is not mentioned in the reference text. Since the reference text does not confirm the exact length given in the answer, the answer is considered to be a hallucination of facts, as it assumes information not available in the reference text.   \n",
       "2  To determine if the answer is factual or hallucinated, we need to compare the information in the answer with the reference text. The query asks about the largest tropical rainforest. The reference text states that the Amazon rainforest is the largest tropical rainforest in the world, which matches the first part of the answer. However, the answer also claims that the Amazon rainforest is home to the largest number of plant and animal species in the world. This information is not mentioned in the reference text. The reference text only provides information about the size and location of the Amazon rainforest, not about the biodiversity or the number of species it contains. Therefore, the claim about the largest number of plant and animal species is not supported by the reference text and is considered a hallucination.   \n",
       "3                                                                                                                                                 First, we identify the query, which asks for the highest mountain on Earth. The reference text clearly states that Mount Everest is the highest mountain on Earth, located in the Mahalangur Himal sub-range of the Himalayas, straddling the border between Nepal and Tibet. The answer provided states that Mount Everest is the highest mountain on Earth and adds specific details about its height, 29,029 feet (8,848 meters). The reference text does not provide the height of Mount Everest, so the specific height mentioned in the answer is not supported by the reference text. Therefore, the answer includes information that is not present in the reference text, making it a hallucination.   \n",
       "4                                                                                                                                                                                        To determine if the answer is factual or hallucinated, we need to compare the information in the answer with the reference text. The query asks for the longest river in the world. The reference text states that the Nile is the longest river in the world, flowing for approximately 6,650 km (4,132 miles). The answer also states that the Nile River is the longest river in the world and provides the same length of 6,650 kilometers (4,132 miles). Since the answer matches the information provided in the reference text, it does not contain any false information or assumptions not supported by the reference text. Therefore, the answer is factual.   \n",
       "\n",
       "   qa_eval  \\\n",
       "0  correct   \n",
       "1  correct   \n",
       "2  correct   \n",
       "3  correct   \n",
       "4  correct   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             qa_explanation  \n",
       "0                                                                                                                                                                                                                          To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. \\n\\n1. The question asks for the location of the Eiffel Tower.\\n2. The reference text states that \"The Eiffel Tower is located in Paris, France.\"\\n3. The answer provided is \"The Eiffel Tower is located in Paris, France.\"\\n\\nThe answer matches exactly with the information given in the reference text regarding the location of the Eiffel Tower. Therefore, the answer correctly and fully answers the question based on the reference text.  \n",
       "1  To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. \\n\\n1. The question asks about the length of the Great Wall of China.\\n2. The reference text states that the Great Wall of China is over 13,000 miles long.\\n3. The answer states that the Great Wall of China is approximately 13,171 miles (21,196 kilometers) long.\\n4. The answer provides a more precise figure (13,171 miles) which is consistent with the reference text's statement that the wall is over 13,000 miles long.\\n5. The reference text does not contradict the answer, and the answer provides additional detail that is plausible and aligns with the reference.\\n\\nTherefore, the answer correctly answers the question based on the reference text.  \n",
       "2                                     To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. The question asks for the largest tropical rainforest. The reference text states that the Amazon rainforest is the largest tropical rainforest in the world. The answer also states that the Amazon rainforest is the largest tropical rainforest in the world, which directly matches the information in the reference text. Additionally, the answer provides extra information about the Amazon rainforest being home to the largest number of plant and animal species, but this does not contradict the reference text or the question. Therefore, the answer correctly and fully answers the question based on the reference text.  \n",
       "3                                                                                                                             To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. The question asks for the highest mountain on Earth. The reference text states that Mount Everest is the highest mountain on Earth. The answer also states that Mount Everest is the highest mountain on Earth and provides additional information about its height, which is consistent with known data. The additional detail about the height does not contradict the reference text and is accurate. Therefore, the answer correctly identifies Mount Everest as the highest mountain on Earth, which is consistent with the reference text.  \n",
       "4                                                                                                         To determine if the answer is correct, we need to compare the information provided in the answer with the information in the reference text. \\n\\n1. The question asks for the longest river in the world.\\n2. The reference text states that the Nile is the longest river in the world, with a length of approximately 6,650 km (4,132 miles).\\n3. The answer states that the Nile River, at 6,650 kilometers (4,132 miles), is the longest river in the world.\\n\\nThe answer correctly identifies the Nile River as the longest river in the world and provides the same length as mentioned in the reference text. Therefore, the answer is consistent with the reference text and fully answers the question.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df = df.copy()\n",
    "results_df[\"hallucination_eval\"] = hallucination_eval_df[\"label\"]\n",
    "results_df[\"hallucination_explanation\"] = hallucination_eval_df[\"explanation\"]\n",
    "results_df[\"qa_eval\"] = qa_eval_df[\"label\"]\n",
    "results_df[\"qa_explanation\"] = qa_eval_df[\"explanation\"]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3278f6-a626-47a1-b405-c0d5e220d3d9",
   "metadata": {},
   "source": [
    "# Evals with Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f18ada-0f8b-461a-8a41-6605b8a3a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `model_name` field is deprecated. Use `model` instead.                 This will be removed in a future release.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/_4jfr9sn6m3_8lbtyt_p2fm80000gn/T/ipykernel_43453/1106521010.py:18: DeprecationWarning: `dataframe` argument is deprecated; use `data` instead\n",
      "  relevance_classifications = llm_classify(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476af8bd5bc44711b346c9eafaeaf0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from phoenix.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "model = OpenAIModel(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "#The rails is used to hold the output to specific values based on the template\n",
    "#It will remove text such as \",,,\" or \"...\"\n",
    "#Will ensure the binary value expected from the template is returned\n",
    "rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
    "relevance_classifications = llm_classify(\n",
    "    dataframe=df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    provide_explanation=True\n",
    ")\n",
    "#relevance_classifications is a Dataframe with columns 'label' and 'explanation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0adbcda-859c-4cbb-b553-ae1eda0291ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relevant', 'unrelated']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b17916-7a9f-4e3d-be5a-1ac78a39bf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks for the location of the Eiffel Tower. The reference text provides this information by stating that the Eiffel Tower is located in Paris, France. Therefore, the reference text is relevant to the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.168204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks for the length of the Great Wall of China. The reference text provides this information directly by stating that the Great Wall of China is over 13,000 miles long. Therefore, the reference text is relevant to the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.480344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks for the largest tropical rainforest. The reference text directly provides this information by stating that the Amazon rainforest is the largest tropical rainforest in the world. Therefore, the reference text is relevant to the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.742180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks for the highest mountain on Earth. The reference text directly provides this information, stating that Mount Everest is the highest mountain on Earth. Therefore, the reference text is relevant to the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.639431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks for the longest river in the world. The reference text directly provides this information, stating that the Nile is the longest river in the world. Therefore, the reference text is relevant to the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.672552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks who painted the Mona Lisa. The reference text directly answers this question by stating that the Mona Lisa was painted by Leonardo da Vinci. Therefore, the reference text is relevant to the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.761424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks for the number of bones in the human body. The reference text provides this information directly by stating that the human body has 206 bones. Therefore, the reference text is relevant to the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.773140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks for the largest planet in our solar system. The reference text directly provides this information by stating that Jupiter is the largest planet in our solar system. Therefore, the reference text is relevant to the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.853509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks for the author of 'Romeo and Juliet'. The reference text clearly states that William Shakespeare wrote 'Romeo and Juliet'. Therefore, the reference text contains information that directly answers the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>3.366698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relevant</td>\n",
       "      <td>The question asks for the date of the first moon landing. The reference text provides this information, stating that the first moon landing occurred on July 20, 1969. Therefore, the reference text is relevant to the question.</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>3.474249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  relevant   \n",
       "1  relevant   \n",
       "2  relevant   \n",
       "3  relevant   \n",
       "4  relevant   \n",
       "5  relevant   \n",
       "6  relevant   \n",
       "7  relevant   \n",
       "8  relevant   \n",
       "9  relevant   \n",
       "\n",
       "                                                                                                                                                                                                                                                      explanation  \\\n",
       "0                                   The question asks for the location of the Eiffel Tower. The reference text provides this information by stating that the Eiffel Tower is located in Paris, France. Therefore, the reference text is relevant to the question.   \n",
       "1                The question asks for the length of the Great Wall of China. The reference text provides this information directly by stating that the Great Wall of China is over 13,000 miles long. Therefore, the reference text is relevant to the question.   \n",
       "2  The question asks for the largest tropical rainforest. The reference text directly provides this information by stating that the Amazon rainforest is the largest tropical rainforest in the world. Therefore, the reference text is relevant to the question.   \n",
       "3                             The question asks for the highest mountain on Earth. The reference text directly provides this information, stating that Mount Everest is the highest mountain on Earth. Therefore, the reference text is relevant to the question.   \n",
       "4                                The question asks for the longest river in the world. The reference text directly provides this information, stating that the Nile is the longest river in the world. Therefore, the reference text is relevant to the question.   \n",
       "5                                       The question asks who painted the Mona Lisa. The reference text directly answers this question by stating that the Mona Lisa was painted by Leonardo da Vinci. Therefore, the reference text is relevant to the question.   \n",
       "6                                     The question asks for the number of bones in the human body. The reference text provides this information directly by stating that the human body has 206 bones. Therefore, the reference text is relevant to the question.   \n",
       "7               The question asks for the largest planet in our solar system. The reference text directly provides this information by stating that Jupiter is the largest planet in our solar system. Therefore, the reference text is relevant to the question.   \n",
       "8                             The question asks for the author of 'Romeo and Juliet'. The reference text clearly states that William Shakespeare wrote 'Romeo and Juliet'. Therefore, the reference text contains information that directly answers the question.   \n",
       "9                               The question asks for the date of the first moon landing. The reference text provides this information, stating that the first moon landing occurred on July 20, 1969. Therefore, the reference text is relevant to the question.   \n",
       "\n",
       "  exceptions execution_status  execution_seconds  \n",
       "0         []        COMPLETED           2.168204  \n",
       "1         []        COMPLETED           2.480344  \n",
       "2         []        COMPLETED           2.742180  \n",
       "3         []        COMPLETED           2.639431  \n",
       "4         []        COMPLETED           2.672552  \n",
       "5         []        COMPLETED           2.761424  \n",
       "6         []        COMPLETED           2.773140  \n",
       "7         []        COMPLETED           2.853509  \n",
       "8         []        COMPLETED           3.366698  \n",
       "9         []        COMPLETED           3.474249  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055efd8-a242-4e84-805b-73b79672408c",
   "metadata": {},
   "source": [
    "# Custom Task Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601bc4cd-ca14-457f-8891-857c68b41f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "      <th>is_hallucination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>() is a prefecture-level city in northwestern Anhui province, China.Gaozhou is a county-level city in southwestern Guangdong Province, China.</td>\n",
       "      <td>Can  Fuyang and Gaozhou be found in the same province?</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>() is a prefecture-level city in northwestern Anhui province, China.Gaozhou is a county-level city in southwestern Guangdong Province, China.</td>\n",
       "      <td>Can  Fuyang and Gaozhou be found in the same province?</td>\n",
       "      <td>Yes, Fuyang and Gaozhou are in the same province.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"808\" was a success in the United States becoming the group's first top ten hit peaking at number eight on the \"Billboard\" Hot 100 and top five peaking at number four on the Hot R&amp;B/Hip-Hop Singles.The \"Billboard\" Hot 100 is the music industry standard record chart in the United States for singles, published weekly by \"Billboard\" magazine.</td>\n",
       "      <td>808 peaked at number eight on what?</td>\n",
       "      <td>Billboard\" Hot 100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"808\" was a success in the United States becoming the group's first top ten hit peaking at number eight on the \"Billboard\" Hot 100 and top five peaking at number four on the Hot R&amp;B/Hip-Hop Singles.The \"Billboard\" Hot 100 is the music industry standard record chart in the United States for singles, published weekly by \"Billboard\" magazine.</td>\n",
       "      <td>808 peaked at number eight on what?</td>\n",
       "      <td>\"808\" peaked at number nine on \"Billboard\" Hot 100.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Arms\" then made a comeback in 2017 reaching #36 on the iTunes chart passing Auli'i Cravalho's \"How Far I'll Go\" from the Disney movie \"Moana\" (2017).Moana ( ) is a 2016 American 3D computer-animated musical fantasy-adventure film produced by Walt Disney Animation Studios and released by Walt Disney Pictures.</td>\n",
       "      <td>Arms is a song by American singer-songwriter Christina Perri, in 2017, it passed Auli'i Cravalho's, \"How Far I'll Go\" from which 2016, American 3D computer-animated Disney movie?</td>\n",
       "      <td>Moana</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                reference  \\\n",
       "0                                                                                                                                                                                                           () is a prefecture-level city in northwestern Anhui province, China.Gaozhou is a county-level city in southwestern Guangdong Province, China.   \n",
       "1                                                                                                                                                                                                           () is a prefecture-level city in northwestern Anhui province, China.Gaozhou is a county-level city in southwestern Guangdong Province, China.   \n",
       "2   \"808\" was a success in the United States becoming the group's first top ten hit peaking at number eight on the \"Billboard\" Hot 100 and top five peaking at number four on the Hot R&B/Hip-Hop Singles.The \"Billboard\" Hot 100 is the music industry standard record chart in the United States for singles, published weekly by \"Billboard\" magazine.   \n",
       "3   \"808\" was a success in the United States becoming the group's first top ten hit peaking at number eight on the \"Billboard\" Hot 100 and top five peaking at number four on the Hot R&B/Hip-Hop Singles.The \"Billboard\" Hot 100 is the music industry standard record chart in the United States for singles, published weekly by \"Billboard\" magazine.   \n",
       "4                                  \"Arms\" then made a comeback in 2017 reaching #36 on the iTunes chart passing Auli'i Cravalho's \"How Far I'll Go\" from the Disney movie \"Moana\" (2017).Moana ( ) is a 2016 American 3D computer-animated musical fantasy-adventure film produced by Walt Disney Animation Studios and released by Walt Disney Pictures.   \n",
       "\n",
       "                                                                                                                                                                                query  \\\n",
       "0                                                                                                                              Can  Fuyang and Gaozhou be found in the same province?   \n",
       "1                                                                                                                              Can  Fuyang and Gaozhou be found in the same province?   \n",
       "2                                                                                                                                                 808 peaked at number eight on what?   \n",
       "3                                                                                                                                                 808 peaked at number eight on what?   \n",
       "4  Arms is a song by American singer-songwriter Christina Perri, in 2017, it passed Auli'i Cravalho's, \"How Far I'll Go\" from which 2016, American 3D computer-animated Disney movie?   \n",
       "\n",
       "                                              response  is_hallucination  \n",
       "0                                                   no             False  \n",
       "1    Yes, Fuyang and Gaozhou are in the same province.              True  \n",
       "2                                   Billboard\" Hot 100             False  \n",
       "3  \"808\" peaked at number nine on \"Billboard\" Hot 100.              True  \n",
       "4                                                Moana             False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = download_benchmark_dataset(\n",
    "    task=\"binary-hallucination-classification\", dataset_name=\"halueval_qa_data\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fa3eaa0-80db-4e96-8d41-a2a1a177ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_CUSTOM_TEMPLATE = '''\n",
    "    You are evaluating the positivity or negativity of the responses to questions.\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {query}\n",
    "    ************\n",
    "    [Response]: {response}\n",
    "    [END DATA]\n",
    "\n",
    "\n",
    "    Please focus on the tone of the response.\n",
    "    Your answer must be single word, either \"positive\" or \"negative\"\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbc569b-1683-4c91-9060-630a7cb5e8e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAIModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#model = OpenAIModel(model_name=\"gpt-4\",temperature=0.6)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mOpenAIModel\u001b[49m(\n\u001b[32m      3\u001b[39m             api_key=\u001b[33m\"\u001b[39m\u001b[33md7817408215242db91a1fea3b2bbc95c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m             azure_endpoint=\u001b[33m\"\u001b[39m\u001b[33mhttps://sonderwestus.openai.azure.com\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m             model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m             api_version=\u001b[33m\"\u001b[39m\u001b[33m2023-05-15\u001b[39m\u001b[33m\"\u001b[39m,  \n\u001b[32m      7\u001b[39m             azure_deployment=\u001b[33m\"\u001b[39m\u001b[33mdev-gpt-4o\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m         )\n\u001b[32m      9\u001b[39m positive_eval = llm_classify(\n\u001b[32m     10\u001b[39m     dataframe=df,\n\u001b[32m     11\u001b[39m     template= MY_CUSTOM_TEMPLATE,\n\u001b[32m     12\u001b[39m     model=model,\n\u001b[32m     13\u001b[39m     rails=rails\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'OpenAIModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = OpenAIModel(model_name=\"gpt-4\",temperature=0.6)\n",
    "\n",
    "positive_eval = llm_classify(\n",
    "    dataframe=df,\n",
    "    template= MY_CUSTOM_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e00e1-7b8a-45c3-93df-5ab358fa8301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
